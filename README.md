# ollama-intelligence-chat ü§ñ

A robust, local AI assistant built using **LangChain**,**Streamlit**, and **Ollama**. This project demonstrates how to orchestrate local Large Language Models (LLMs) to perform complex tasks while maintaining 100% data privacy.

## üöÄ Features
* **Interactive UI:** Built with **Streamlit** for a smooth, user-friendly chat experience.
* **Local Inference:** Powered by **Gemma 2:2B** via Ollama for fast, private processing.
* **LangChain Orchestration:** Utilizes LangChain for structured prompt handling and conversation memory.
* **Optimized Performance:** Specifically configured to run efficiently on standard consumer hardware.
* **Privacy-First:** All data remains on the local machine; no API keys or external cloud processing required.

## üõ†Ô∏è Tech Stack
* **UI Framework:** Streamlit
* **LLM Framework:** LangChain
* **Inference Engine:** Ollama
* **Model:** Gemma 2:2B (Google Open Weights)
* **Language:** Python 3.10+

## üìã Prerequisites
1. **Install Ollama:** [ollama.com](https://ollama.com/)
2. **Download the Model:** `ollama pull gemma2:2b`
